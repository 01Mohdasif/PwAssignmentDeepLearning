{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ForwardandBackwardPropagation**"
      ],
      "metadata": {
        "id": "OKRk9jn_PSHU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: Explain the concept of forward propagation in a neural network.\n",
        "\"\"\"\n",
        "Answer:\n",
        "Forward propagation is the process by which an input is passed through a neural network to generate an output. In a typical feedforward neural network, forward propagation involves moving the input through multiple layers, where each layer performs some computation and passes its result to the next layer.\n",
        "\n",
        "In the case of a neural network with one or more hidden layers:\n",
        "1. The input is multiplied by the weights (which are initially random).\n",
        "2. A bias term is added to this weighted sum.\n",
        "3. An activation function is applied to the result to introduce non-linearity.\n",
        "4. This process is repeated layer by layer, until the final output layer is reached, producing the networkâ€™s prediction.\n",
        "\n",
        "Mathematically:\n",
        "- For each layer \\( i \\), the output is computed as:\n",
        "  \\[\n",
        "  \\mathbf{a}_i = \\text{activation}(\\mathbf{W}_i \\mathbf{a}_{i-1} + \\mathbf{b}_i)\n",
        "  \\]\n",
        "  where \\( \\mathbf{a}_{i-1} \\) is the output from the previous layer (starting with the input layer), \\( \\mathbf{W}_i \\) are the weights, and \\( \\mathbf{b}_i \\) is the bias.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "sSs2KlknPUQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the purpose of the activation function in forward propagation?\n",
        "\"\"\"\n",
        "Answer:\n",
        "The activation function introduces non-linearity into the network. Without activation functions, the neural network would behave like a linear regression model, regardless of the number of layers. This is because a composition of linear functions is still a linear function.\n",
        "\n",
        "Activation functions allow the network to model complex relationships between input and output, making it capable of solving more complex tasks, such as image classification, object detection, etc.\n",
        "\n",
        "Common activation functions include:\n",
        "1. **Sigmoid**: Outputs values between 0 and 1, used primarily in binary classification.\n",
        "2. **ReLU (Rectified Linear Unit)**: Outputs values greater than or equal to 0, used to introduce sparsity and avoid vanishing gradients.\n",
        "3. **Tanh (Hyperbolic Tangent)**: Outputs values between -1 and 1, often used in hidden layers.\n",
        "\n",
        "The choice of activation function affects the learning and performance of the model.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "Fz1DhcsQPY8g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Describe the steps involved in the backward propagation (backpropagation) algorithm.\n",
        "\"\"\"\n",
        "Answer:\n",
        "Backpropagation is the process used to train neural networks by adjusting the weights based on the error (or loss) of the output. The steps involved are:\n",
        "\n",
        "1. **Calculate the Loss**: The first step is to calculate the error or loss using a loss function (e.g., mean squared error or cross-entropy). This is computed as the difference between the predicted output and the actual target.\n",
        "   \n",
        "2. **Backward Pass (Error Propagation)**:\n",
        "   - Starting from the output layer, backpropagate the error backwards through the network.\n",
        "   - For each layer, compute the gradient of the loss with respect to the weights and biases using the chain rule.\n",
        "\n",
        "3. **Update Weights and Biases**: Once the gradients are computed, update the weights and biases using an optimization algorithm like gradient descent.\n",
        "\n",
        "The goal of backpropagation is to minimize the loss function, ultimately allowing the neural network to make better predictions.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "7_x5gtRgPbm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What is the purpose of the chain rule in backpropagation?\n",
        "\"\"\"\n",
        "Answer:\n",
        "The chain rule is a fundamental concept in calculus used to compute derivatives of composite functions. In the context of backpropagation, the chain rule allows us to compute the gradient of the loss function with respect to each parameter (weight and bias) in the neural network.\n",
        "\n",
        "The backpropagation algorithm involves computing gradients layer by layer. The chain rule is used to propagate the error backward through the network, starting from the output layer and going toward the input layer.\n",
        "\n",
        "Mathematically, if the loss function is \\( L \\), the weight updates for a layer \\( k \\) depend on the gradient of \\( L \\) with respect to the weight at layer \\( k \\). Using the chain rule, we can express the gradient as:\n",
        "\\[\n",
        "\\frac{\\partial L}{\\partial W_k} = \\frac{\\partial L}{\\partial a_k} \\cdot \\frac{\\partial a_k}{\\partial z_k} \\cdot \\frac{\\partial z_k}{\\partial W_k}\n",
        "\\]\n",
        "where \\( a_k \\) is the activation of layer \\( k \\), and \\( z_k \\) is the input to the activation function in layer \\( k \\). The chain rule ensures that the error is appropriately propagated and used to update the parameters in the network.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "o-JSc-m6Pd1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Implement the forward propagation process for a simple neural network with one hidden layer using NumPy\n",
        "\n",
        "#Answer:In this example, we will implement forward propagation for a simple neural network with one hidden layer using NumPy.\n",
        "\n",
        " #Let's assume:\n",
        "#- The input layer has 3 features (3 input nodes).\n",
        "#- The hidden layer has 4 neurons.\n",
        "#- The output layer has 2 neurons (for binary classification or multi-class classification with 2 classes).\n",
        "\n",
        "# We will use ReLU activation for the hidden layer and softmax activation for the output layer.\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Define the input features (3 features)\n",
        "X = np.array([[0.5, 0.6, 0.1]])\n",
        "\n",
        "# Define the weights and biases\n",
        "# Input to hidden layer weights (3 input nodes, 4 hidden neurons)\n",
        "W1 = np.random.randn(3, 4)\n",
        "b1 = np.random.randn(1, 4)\n",
        "\n",
        "# Hidden to output layer weights (4 hidden neurons, 2 output neurons)\n",
        "W2 = np.random.randn(4, 2)\n",
        "b2 = np.random.randn(1, 2)\n",
        "\n",
        "# Forward Propagation\n",
        "\n",
        "# 1. Compute hidden layer activations (using ReLU)\n",
        "Z1 = np.dot(X, W1) + b1  # Linear transformation\n",
        "A1 = np.maximum(0, Z1)   # ReLU activation\n",
        "\n",
        "# 2. Compute output layer activations (using softmax)\n",
        "Z2 = np.dot(A1, W2) + b2  # Linear transformation\n",
        "A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=1, keepdims=True)  # Softmax activation\n",
        "\n",
        "print(\"Output of the network: \", A2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8DXZ5DBPhMn",
        "outputId": "e03e14f3-d433-48ac-ee94-c35c69a26b54"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output of the network:  [[0.02914407 0.97085593]]\n"
          ]
        }
      ]
    }
  ]
}