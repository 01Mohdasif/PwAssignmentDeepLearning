{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 1: What is the primary goal of Natural Language Processing (NLP)?**\n",
        "# Answer:\n",
        "# The primary goal of Natural Language Processing (NLP) is to enable machines to understand, interpret, and generate human\n",
        "# language in a way that is both meaningful and useful. It involves tasks such as text classification, sentiment analysis,\n",
        "# machine translation, and question-answering, with the aim of improving human-computer interaction through language.\n"
      ],
      "metadata": {
        "id": "OB4QclR3Gp4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 2: What does \"tokenization\" refer to in text processing?**\n",
        "# Answer:\n",
        "# Tokenization refers to the process of splitting a text into smaller units, typically words or sentences, called tokens.\n",
        "# This is a crucial step in text processing that allows machines to analyze and process text more effectively.\n"
      ],
      "metadata": {
        "id": "uTjhxstNGqTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 3: What is the difference between lemmatization and stemming?**\n",
        "# Answer:\n",
        "# **Stemming** is the process of reducing words to their root form by removing suffixes, but it may not result in a valid word.\n",
        "# **Lemmatization**, on the other hand, reduces words to their base or dictionary form (lemma) and ensures the resulting word\n",
        "# is valid. Lemmatization is more accurate and context-sensitive than stemming.\n"
      ],
      "metadata": {
        "id": "sw6uCHsUGsav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 4: What is the role of regular expressions (regex) in text processing?**\n",
        "# Answer:\n",
        "# Regular expressions (regex) are used to define search patterns for matching specific strings in text. They are essential\n",
        "# for tasks such as text cleaning, pattern matching, and extraction of specific information (like phone numbers, email addresses, etc.).\n"
      ],
      "metadata": {
        "id": "2KilBN8oGwS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 5: What is Word2Vec and how does it represent words in a vector space?**\n",
        "# Answer:\n",
        "# Word2Vec is a popular technique for word embedding that represents words in a continuous vector space. It uses neural\n",
        "# networks to learn word representations where similar words are closer in the vector space, capturing semantic relationships\n",
        "# between words. Word2Vec typically uses either the Skip-gram or Continuous Bag of Words (CBOW) model.\n"
      ],
      "metadata": {
        "id": "yjLFWsrNG-M4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 6: How does frequency distribution help in text analysis?**\n",
        "# Answer:\n",
        "# Frequency distribution in text analysis helps identify the most common words or phrases in a dataset. It provides insights\n",
        "# into the importance or relevance of terms, helping with tasks like feature selection, keyword extraction, and document\n",
        "# classification.\n"
      ],
      "metadata": {
        "id": "K6UeABb-G_5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 7: Why is text normalization important in NLP?**\n",
        "# Answer:\n",
        "# Text normalization is crucial in NLP as it standardizes text by removing inconsistencies such as case sensitivity,\n",
        "# punctuation, and irrelevant characters. This makes it easier for models to process and analyze the data consistently.\n"
      ],
      "metadata": {
        "id": "cuxQqVvVHEUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 8: What is the difference between sentence tokenization and word tokenization?**\n",
        "# Answer:\n",
        "# **Sentence tokenization** is the process of dividing a text into individual sentences, while **word tokenization** breaks\n",
        "# text into individual words. Sentence tokenization is typically the first step in text processing before word tokenization.\n"
      ],
      "metadata": {
        "id": "qeH_n9cTHJ9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 9: What are co-occurrence vectors in NLP?**\n",
        "# Answer:\n",
        "# Co-occurrence vectors represent the frequency of words appearing together in a given context (like a sentence or a window of\n",
        "# surrounding words). These vectors help capture semantic relationships between words, which can be useful for tasks like\n",
        "# word similarity and document clustering.\n"
      ],
      "metadata": {
        "id": "2UfSZ4lUHMf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TjIVO3ybHOP9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 10: What is the significance of lemmatization in improving NLP tasks?**\n",
        "# Answer:\n",
        "# Lemmatization improves NLP tasks by ensuring words are reduced to their base forms, which enhances accuracy in tasks like\n",
        "# text classification, sentiment analysis, and machine translation. Unlike stemming, lemmatization retains the meaning of the word.\n"
      ],
      "metadata": {
        "id": "mYyh_RoQHP7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 11: What is the primary use of word embeddings in NLP?**\n",
        "# Answer:\n",
        "# Word embeddings represent words as dense vectors in a continuous vector space. Their primary use is to capture the\n",
        "# semantic meaning of words and the relationships between them, helping models understand context, similarity, and meaning in\n",
        "# tasks like translation, sentiment analysis, and information retrieval.\n"
      ],
      "metadata": {
        "id": "F5s4w42EHQt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 12: What is an annotator in NLP?**\n",
        "# Answer:\n",
        "# An **annotator** in NLP refers to a tool or system used to label or annotate data with useful information such as part of\n",
        "# speech, named entities, or syntactic dependencies. This annotated data is essential for supervised learning and model training.\n"
      ],
      "metadata": {
        "id": "qL6J2u_WHSto"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 13: What are the key steps in text processing before applying machine learning models?**\n",
        "# Answer:\n",
        "# Key steps in text processing include:\n",
        "# 1. **Text Cleaning**: Removing noise, stop words, punctuation, and special characters.\n",
        "# 2. **Tokenization**: Breaking down the text into tokens.\n",
        "# 3. **Text Normalization**: Standardizing text through techniques like lowercasing and lemmatization.\n",
        "# 4. **Vectorization**: Converting text into numerical representations like TF-IDF or word embeddings.\n"
      ],
      "metadata": {
        "id": "XNxPTnSMHUUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 14: What is the history of NLP and how has it evolved?**\n",
        "# Answer:\n",
        "# NLP has evolved from early rule-based systems in the 1950s and 1960s, to statistical methods in the 1990s, and now deep\n",
        "# learning-based models. Advances in neural networks, such as transformer models (e.g., BERT, GPT), have significantly improved\n",
        "# NLP's ability to understand and generate human language in recent years.\n"
      ],
      "metadata": {
        "id": "M3e1XnJTHXKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 15: Why is sentence processing important in NLP?**\n",
        "# Answer:\n",
        "# Sentence processing is important in NLP because it allows for the understanding of relationships between words in context.\n",
        "# It aids in tasks such as sentiment analysis, syntactic parsing, machine translation, and summarization by capturing sentence-level structure.\n"
      ],
      "metadata": {
        "id": "o592WvgeHY_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 16: How do word embeddings improve the understanding of language semantics in NLP?**\n",
        "# Answer:\n",
        "# Word embeddings improve the understanding of language semantics by mapping words into continuous vector spaces, where words\n",
        "# with similar meanings are placed close together. This captures nuanced relationships like synonyms, antonyms, and context,\n",
        "# which enhances NLP models' ability to perform tasks such as sentiment analysis, machine translation, and document classification.\n"
      ],
      "metadata": {
        "id": "zzHKwfikHbEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 17: How does the frequency distribution of words help in text classification?**\n",
        "# Answer:\n",
        "# The frequency distribution of words helps identify important words that are characteristic of different classes in text\n",
        "# classification tasks. Words that appear frequently in one class may be used as features to predict the class of unseen text\n",
        "# documents.\n"
      ],
      "metadata": {
        "id": "mzkpen2LHcm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 18: What are the advantages of using regex in text cleaning?**\n",
        "# Answer:\n",
        "# The advantages of using regular expressions (regex) in text cleaning include:\n",
        "# 1. **Flexibility**: Regex allows for complex and flexible pattern matching to handle various text patterns.\n",
        "# 2. **Efficiency**: It can quickly remove unwanted characters or extract specific information from large amounts of text.\n",
        "# 3. **Customization**: Regex can be tailored to match the specific structure of the text you're working with, ensuring precision.\n"
      ],
      "metadata": {
        "id": "FxzuQtOyHeIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 19: What is the difference between Word2Vec and Doc2Vec?**\n",
        "# Answer:\n",
        "# Word2Vec is designed to represent individual words in a continuous vector space, capturing the semantic meaning of words.\n",
        "# Doc2Vec, on the other hand, is an extension of Word2Vec and represents entire documents or paragraphs as vectors,\n",
        "# capturing the context and semantic information of larger bodies of text.\n"
      ],
      "metadata": {
        "id": "2fTrbp8mH0gI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 20: Why is understanding text normalization important in NLP?**\n",
        "# Answer:\n",
        "# Text normalization is important because it standardizes raw text and removes inconsistencies, such as varying word forms,\n",
        "# punctuation, and case sensitivity. It ensures that text is processed consistently, making it easier for NLP models to\n",
        "# analyze and extract relevant features.\n"
      ],
      "metadata": {
        "id": "KIIjVM2IH03B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 21: How does word count help in text analysis?**\n",
        "# Answer:\n",
        "# Word count helps in text analysis by providing a measure of the frequency of specific words or terms in a document or corpus.\n",
        "# It is commonly used in tasks like feature extraction for text classification, sentiment analysis, and topic modeling, as frequently\n",
        "# occurring words can indicate important features or topics.\n"
      ],
      "metadata": {
        "id": "VWocTjI0H2j5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 22: How does lemmatization help in NLP tasks like search engines and chatbots?**\n",
        "# Answer:\n",
        "# Lemmatization helps improve the accuracy of NLP tasks like search engines and chatbots by reducing words to their base form\n",
        "# (lemma). This ensures that different forms of a word (e.g., \"run\" and \"running\") are treated as the same, improving search results\n",
        "# and conversation flow.\n"
      ],
      "metadata": {
        "id": "0sOrDNUWH4NM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 23: What is the purpose of using Doc2Vec in text processing?**\n",
        "# Answer:\n",
        "# Doc2Vec is used to represent entire documents or text passages as dense vectors, capturing the semantic meaning and context of\n",
        "# the document. This enables tasks like document similarity, clustering, and retrieval, where understanding the relationship\n",
        "# between documents is crucial.\n"
      ],
      "metadata": {
        "id": "lErvNBLsH52V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 24: What is the importance of sentence processing in NLP?**\n",
        "# Answer:\n",
        "# Sentence processing is important in NLP as it enables models to understand sentence structure, context, and meaning. This is\n",
        "# essential for tasks like machine translation, sentiment analysis, and question answering, where understanding sentence-level\n",
        "# nuances is required for accurate results.\n"
      ],
      "metadata": {
        "id": "bBI9ZNwUH7gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 25: What is text normalization, and what are the common techniques used in it?**\n",
        "# Answer:\n",
        "# Text normalization refers to the process of standardizing and cleaning raw text to make it more uniform for analysis.\n",
        "# Common techniques include converting text to lowercase, removing punctuation, stemming or lemmatization, and correcting spelling\n",
        "# errors.\n"
      ],
      "metadata": {
        "id": "yw-Z0vktH85_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 26: Why is word tokenization important in NLP?**\n",
        "# Answer:\n",
        "# Word tokenization is important because it breaks down a text into smaller units, specifically words. These units are necessary\n",
        "# for tasks like text classification, sentiment analysis, and named entity recognition, as the model needs to work with individual words.\n"
      ],
      "metadata": {
        "id": "S2lL7Bh3H-TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 27: How does sentence tokenization differ from word tokenization in NLP?**\n",
        "# Answer:\n",
        "# Sentence tokenization splits a text into individual sentences, while word tokenization divides the text into individual words.\n",
        "# Sentence tokenization provides higher-level structural information, useful for tasks like summarization or sentence-level analysis.\n"
      ],
      "metadata": {
        "id": "Wh24rv5SH_4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 28: What is the primary purpose of text processing in NLP?**\n",
        "# Answer:\n",
        "# The primary purpose of text processing in NLP is to prepare raw text data for analysis by cleaning, normalizing, and structuring it.\n",
        "# This is crucial for downstream tasks such as feature extraction, text classification, and language modeling.\n"
      ],
      "metadata": {
        "id": "YSNAIsWAICiB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 29: What are the key challenges in NLP?**\n",
        "# Answer:\n",
        "# Key challenges in NLP include ambiguity (words with multiple meanings), context understanding, handling diverse language\n",
        "# structures, dealing with large vocabularies, and ensuring accurate interpretation of sentiment and intent in real-world text.\n"
      ],
      "metadata": {
        "id": "a-fPDDOrID3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 30: How do co-occurrence vectors represent relationships between words?**\n",
        "# Answer:\n",
        "# Co-occurrence vectors represent relationships by counting how often words appear together in a given context, such as a sliding\n",
        "# window around a target word. Words that frequently occur together are considered related and thus are represented by similar vectors.\n"
      ],
      "metadata": {
        "id": "JZBAPlugIFQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 31: What is the role of frequency distribution in text analysis?**\n",
        "# Answer:\n",
        "# Frequency distribution helps identify important and frequent words or phrases in a text or corpus. It provides insights into\n",
        "# the content and significance of terms, which can aid in feature selection, keyword extraction, and text classification.\n"
      ],
      "metadata": {
        "id": "5oKxyJcLIGh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 32: What is the impact of word embeddings on NLP tasks?**\n",
        "# Answer:\n",
        "# Word embeddings enhance NLP tasks by mapping words to continuous vector spaces, where similar words are closer in proximity.\n",
        "# This enables NLP models to better understand the relationships between words, improving tasks like translation, sentiment\n",
        "# analysis, and text summarization.\n"
      ],
      "metadata": {
        "id": "2Wt2W80KIH7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SFMAWfdqIoQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question 33: What is the purpose of using lemmatization in text preprocessing?**\n",
        "# Answer:\n",
        "# The purpose of lemmatization in text preprocessing is to reduce words to their base or root form, ensuring that different\n",
        "# inflections or variations of a word (e.g., \"running\" and \"run\") are treated as the same. This reduces the dimensionality of\n",
        "# the feature space and improves the performance of NLP models.\n"
      ],
      "metadata": {
        "id": "eGBw-CAfIJYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PRACTICLE**"
      ],
      "metadata": {
        "id": "MH6XNvJ-IpfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# **1. How can you perform word tokenization using NLTK:**\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Check if the punkt resource is already downloaded, if not, download it\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading 'punkt' resource...\")\n",
        "    nltk.download('punkt')  # Download the punkt tokenizer models if not already downloaded\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dvoAHpqHIrk0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **2. How can you perform sentence tokenization using NLTK:**\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfCOaDYJIsYU",
        "outputId": "b0742ca4-ea2c-41f6-939d-4ddf1cc6cb65"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **3. How can you remove stopwords from a sentence:**\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Delete punkt_tab if it exists\n",
        "punkt_tab_dir = '/root/nltk_data/tokenizers/punkt_tab'\n",
        "if os.path.exists(punkt_tab_dir):\n",
        "    shutil.rmtree(punkt_tab_dir)\n",
        "\n",
        "# Then, re-download the correct punkt tokenizer\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJZANc39It2E",
        "outputId": "5009a9d5-6436-494a-9cd6-510f348bd8f9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example sentence showing stopword removal.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Words without stopwords:\", filtered_words)\n"
      ],
      "metadata": {
        "id": "qanbm1GVLZ_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **4. How can you perform stemming on a word:**\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample word\n",
        "word = \"running\"\n",
        "\n",
        "# Perform stemming\n",
        "stemmed_word = stemmer.stem(word)\n",
        "print(\"Stemmed word:\", stemmed_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZL9LAq_Ived",
        "outputId": "0e9668ba-c77d-4fc3-f8ca-c2c3321c7541"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **5. How can you perform lemmatization on a word:**\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Sample word\n",
        "word = \"running\"\n",
        "\n",
        "# Perform lemmatization\n",
        "lemmatized_word = lemmatizer.lemmatize(word, pos='v')  # Specify part-of-speech (v for verb)\n",
        "print(\"Lemmatized word:\", lemmatized_word)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn5TyzOPIxE4",
        "outputId": "8a32f677-b412-4d0e-cf50-6b6a501be921"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized word: run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **6. How can you normalize a text by converting it to lowercase and removing punctuation:**\n",
        "\n",
        "import string\n",
        "\n",
        "# Sample text\n",
        "text = \"Hello, World! This is an example sentence.\"\n",
        "\n",
        "# Normalize: Convert to lowercase and remove punctuation\n",
        "normalized_text = ''.join([char.lower() for char in text if char not in string.punctuation])\n",
        "print(\"Normalized text:\", normalized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu1r898zIyv3",
        "outputId": "35d7f3a9-4005-4680-f974-1ecc16c8de8c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized text: hello world this is an example sentence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **7. How can you create a co-occurrence matrix for words in a corpus:**\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"This is a sample document.\", \"This document is a sample.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Create a term-document matrix\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to a pandas DataFrame\n",
        "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Create the co-occurrence matrix\n",
        "co_occurrence_matrix = df.T.dot(df)\n",
        "print(\"Co-occurrence matrix:\\n\", co_occurrence_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5jGr2WUI0K1",
        "outputId": "48ae1b7c-4bfd-48a2-fb16-d85095e2a0aa"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Co-occurrence matrix:\n",
            "           document  is  sample  this\n",
            "document         2   2       2     2\n",
            "is               2   2       2     2\n",
            "sample           2   2       2     2\n",
            "this             2   2       2     2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **8. How can you apply a regular expression to extract all email addresses from a text:**\n",
        "\n",
        "import re\n",
        "\n",
        "# Sample text\n",
        "text = \"Please contact us at support@example.com or sales@example.org.\"\n",
        "\n",
        "# Regular expression pattern for email extraction\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b'\n",
        "\n",
        "# Find all email addresses\n",
        "emails = re.findall(email_pattern, text)\n",
        "print(\"Extracted email addresses:\", emails)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QK0z6tDWI1pW",
        "outputId": "80609014-be31-41b5-8a62-8750e5b92fcd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted email addresses: ['support@example.com', 'sales@example.org']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **9. How can you perform word embedding using Word2Vec:**\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample sentences for Word2Vec training\n",
        "sentences = [[\"this\", \"is\", \"a\", \"sample\", \"sentence\"],\n",
        "             [\"word2vec\", \"is\", \"amazing\", \"for\", \"word\", \"embedding\"]]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)\n",
        "\n",
        "# Get the embedding of a word\n",
        "word_embedding = model.wv['sample']\n",
        "print(\"Word embedding for 'sample':\", word_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbPJTxMII3CH",
        "outputId": "262c843a-d700-4cab-d811-fa03b497de4a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word embedding for 'sample': [ 8.1681199e-03 -4.4430327e-03  8.9854337e-03  8.2536647e-03\n",
            " -4.4352221e-03  3.0310510e-04  4.2744912e-03 -3.9263200e-03\n",
            " -5.5599655e-03 -6.5123225e-03 -6.7073823e-04 -2.9592158e-04\n",
            "  4.4630850e-03 -2.4740540e-03 -1.7260908e-04  2.4618758e-03\n",
            "  4.8675989e-03 -3.0808449e-05 -6.3394094e-03 -9.2608072e-03\n",
            "  2.6657581e-05  6.6618943e-03  1.4660227e-03 -8.9665223e-03\n",
            " -7.9386048e-03  6.5519023e-03 -3.7856805e-03  6.2549924e-03\n",
            " -6.6810320e-03  8.4796622e-03 -6.5163244e-03  3.2880199e-03\n",
            " -1.0569858e-03 -6.7875278e-03 -3.2875966e-03 -1.1614120e-03\n",
            " -5.4709399e-03 -1.2113475e-03 -7.5633135e-03  2.6466595e-03\n",
            "  9.0701487e-03 -2.3772502e-03 -9.7651005e-04  3.5135616e-03\n",
            "  8.6650876e-03 -5.9218528e-03 -6.8875779e-03 -2.9329848e-03\n",
            "  9.1476962e-03  8.6626766e-04 -8.6784009e-03 -1.4469790e-03\n",
            "  9.4794659e-03 -7.5494875e-03 -5.3580985e-03  9.3165627e-03\n",
            " -8.9737261e-03  3.8259076e-03  6.6544057e-04  6.6607012e-03\n",
            "  8.3127534e-03 -2.8507852e-03 -3.9923131e-03  8.8979173e-03\n",
            "  2.0896459e-03  6.2489416e-03 -9.4457148e-03  9.5901238e-03\n",
            " -1.3483083e-03 -6.0521150e-03  2.9925345e-03 -4.5661093e-04\n",
            "  4.7064926e-03 -2.2830211e-03 -4.1378425e-03  2.2778988e-03\n",
            "  8.3543835e-03 -4.9956059e-03  2.6686788e-03 -7.9905549e-03\n",
            " -6.7733466e-03 -4.6766878e-04 -8.7677278e-03  2.7894378e-03\n",
            "  1.5985954e-03 -2.3196924e-03  5.0037908e-03  9.7487867e-03\n",
            "  8.4542679e-03 -1.8802249e-03  2.0581519e-03 -4.0036892e-03\n",
            " -8.2414057e-03  6.2779556e-03 -1.9491815e-03 -6.6620467e-04\n",
            " -1.7713320e-03 -4.5356657e-03  4.0617096e-03 -4.2701806e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **10. How can you use Doc2Vec to embed documents:**\n",
        "\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Sample documents\n",
        "documents = [\"This is a sample document.\", \"Word2Vec and Doc2Vec are popular NLP models.\"]\n",
        "\n",
        "# Tagging the documents\n",
        "tagged_documents = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
        "\n",
        "# Train Doc2Vec model\n",
        "model = Doc2Vec(tagged_documents, vector_size=20, window=2, min_count=1, workers=4)\n",
        "\n",
        "# Get document embedding\n",
        "doc_embedding = model.dv['0']\n",
        "print(\"Document embedding for document '0':\", doc_embedding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrbzniG5I4XJ",
        "outputId": "b137bdc9-1220-4adc-d408-99670174454b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document embedding for document '0': [-0.02620433 -0.02993071 -0.04949258  0.04283195  0.01787467  0.00129194\n",
            " -0.04949834 -0.02590201 -0.04864391  0.01003995  0.01419104  0.02327042\n",
            " -0.02153756 -0.01576579 -0.01542092 -0.04366606  0.01087452  0.04616797\n",
            " -0.04758938 -0.01730877]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **11. How can you perform part-of-speech tagging:**\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Step 1: Manually clear any 'punkt_tab' resource that might be causing the issue\n",
        "punkt_tab_dir = '/root/nltk_data/tokenizers/punkt_tab'\n",
        "if os.path.exists(punkt_tab_dir):\n",
        "    shutil.rmtree(punkt_tab_dir)\n",
        "\n",
        "# Step 2: Reset the nltk_data directory by deleting any corrupt or unused resources\n",
        "nltk.data.path = ['/root/nltk_data']  # reset the path to default\n",
        "if os.path.exists('/root/nltk_data'):\n",
        "    shutil.rmtree('/root/nltk_data')  # delete the entire nltk_data folder to ensure a fresh start\n",
        "\n",
        "# Step 3: Re-download required NLTK packages\n",
        "nltk.download('punkt')  # This will download the correct punkt tokenizer\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger resource\n",
        "\n",
        "# Sample text for POS tagging\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Step 4: Tokenize the text and perform POS tagging\n",
        "words = word_tokenize(text)\n",
        "pos_tags = pos_tag(words)\n",
        "\n",
        "# Step 5: Output the result of POS tagging\n",
        "print(\"Part-of-speech tagging:\", pos_tags)\n",
        "\n"
      ],
      "metadata": {
        "id": "XSVoSt5_I5p3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **12. How can you find the similarity between two sentences using cosine similarity:**\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample sentences\n",
        "sentence1 = \"I love programming in Python.\"\n",
        "sentence2 = \"Python is my favorite programming language.\"\n",
        "\n",
        "# Convert sentences to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "\n",
        "# Compute cosine similarity\n",
        "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "print(\"Cosine similarity between the two sentences:\", cos_sim[0][0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gunvnhcjI7K8",
        "outputId": "05bdff57-083a-48fc-9a42-6bae788b2adc"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between the two sentences: 0.26055567105626243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **13. How can you extract named entities from a sentence:**\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the pre-trained SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"Barack Obama was born in Hawaii.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "print(\"Named entities:\", named_entities)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDM6kUg3I8tR",
        "outputId": "75173143-dbaf-4ee3-eee9-7692b176d27f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named entities: [('Barack Obama', 'PERSON'), ('Hawaii', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **14. How can you split a large document into smaller chunks of text:**\n",
        "\n",
        "# Sample large document\n",
        "large_text = \"This is a long document. \" * 10  # Repeated for example\n",
        "\n",
        "# Define chunk size\n",
        "chunk_size = 50  # Split into chunks of 50 characters\n",
        "\n",
        "# Split the document into chunks\n",
        "chunks = [large_text[i:i+chunk_size] for i in range(0, len(large_text), chunk_size)]\n",
        "print(\"Chunks of text:\", chunks[:3])  # Show first 3 chunks\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf2CwWBXI-XR",
        "outputId": "ceb45e27-80f0-4b31-c721-f784979e0a33"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks of text: ['This is a long document. This is a long document. ', 'This is a long document. This is a long document. ', 'This is a long document. This is a long document. ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **15. How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents:**\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert to a DataFrame for readability\n",
        "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", df_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcCiN96aI_sk",
        "outputId": "c4051a86-b319-4604-c45e-d952cac21656"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "        and  document     first        is      one    second       the  \\\n",
            "0  0.00000  0.469417  0.617227  0.364544  0.00000  0.000000  0.364544   \n",
            "1  0.00000  0.728445  0.000000  0.282851  0.00000  0.478909  0.282851   \n",
            "2  0.49712  0.000000  0.000000  0.293607  0.49712  0.000000  0.293607   \n",
            "\n",
            "     third      this  \n",
            "0  0.00000  0.364544  \n",
            "1  0.00000  0.282851  \n",
            "2  0.49712  0.293607  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# **16. How can you apply tokenization, stopword removal, and stemming in one go:**\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Step 1: Ensure that the necessary NLTK corpora are downloaded and available\n",
        "nltk.download('punkt')      # For tokenization\n",
        "nltk.download('stopwords')  # For stopwords\n",
        "\n",
        "# Check if stopwords corpus is available, if not, manually force download\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Step 2: Initialize the stemming tool\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Step 3: Define the text processing function (Tokenization, Stopword Removal, and Stemming)\n",
        "def process_text(text):\n",
        "    words = word_tokenize(text)  # Tokenize the text into words\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]  # Apply stemming\n",
        "    return stemmed_words\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a simple sentence for processing.\"\n",
        "\n",
        "# Step 4: Process the sample text\n",
        "processed_text = process_text(text)\n",
        "\n",
        "# Step 5: Output the result\n",
        "print(\"Processed text:\", processed_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "mWsU_ubfJA-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# **17. How can you visualize the frequency distribution of words in a sentence?**\n",
        "\n",
        "import nltk\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure that 'punkt' is downloaded for tokenization\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a simple example sentence showing word frequency distribution.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Calculate frequency distribution\n",
        "fdist = FreqDist(words)\n",
        "\n",
        "# Plot the frequency distribution\n",
        "fdist.plot()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TRRB0yATJCoD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}